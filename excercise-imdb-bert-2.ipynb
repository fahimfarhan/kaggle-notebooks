{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T18:42:09.853328Z","iopub.execute_input":"2025-03-22T18:42:09.853698Z","iopub.status.idle":"2025-03-22T18:42:11.456989Z","shell.execute_reply.started":"2025-03-22T18:42:09.853667Z","shell.execute_reply":"2025-03-22T18:42:11.456261Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T18:42:11.457771Z","iopub.execute_input":"2025-03-22T18:42:11.458243Z","iopub.status.idle":"2025-03-22T18:42:17.293757Z","shell.execute_reply.started":"2025-03-22T18:42:11.458205Z","shell.execute_reply":"2025-03-22T18:42:17.292736Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import datasets\nimport numpy as np\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport torch.cuda\nimport transformers\nimport wandb\nfrom datasets import DatasetDict\nimport torch.nn.functional as F\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DataCollatorWithPadding, \\\n    TrainingArguments, Trainer\nimport evaluate\n\ndef getCorrectDevice():\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")  # For NVIDIA GPUs\n    elif torch.backends.mps.is_available():\n        return torch.device(\"mps\")  # For Apple Silicon Macs\n    else:\n        return torch.device(\"cpu\")   # Fallback to CPU\n\ndef dynamicBatchSize():\n    if torch.cuda.is_available():\n        gpu_name = torch.cuda.get_device_name(0).lower()\n        vramGiB = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convert to GB\n\n        if \"a100\" in gpu_name:   # A100 (40GB+ VRAM)\n            batch_size = 128\n        elif \"v100\" in gpu_name:  # V100 (16GB/32GB VRAM)\n            batch_size = 64 if vramGiB >= 32 else 32\n        elif \"p100\" in gpu_name:  # P100 (16GB VRAM)\n            batch_size = 32\n        elif \"t4\" in gpu_name:    # Tesla T4 (16GB VRAM, common in Colab/Kaggle)\n            batch_size = 32  # Maybe try 64 if no OOM\n        elif \"rtx 3090\" in gpu_name or vramGiB >= 24:  # RTX 3090 (24GB VRAM)\n            batch_size = 64\n        elif vramGiB >= 16:   # Any other 16GB+ VRAM GPUs\n            batch_size = 32\n        elif vramGiB >= 8:    # 8GB VRAM GPUs (e.g., RTX 2080, 3060, etc.)\n            batch_size = 16\n        elif vramGiB >= 6:    # 6GB VRAM GPUs (e.g., RTX 2060)\n            batch_size = 8\n        else:\n            batch_size = 4  # Safe fallback for smaller GPUs\n    else:\n        batch_size = 4  # CPU mode, keep it small\n\n    return batch_size\n\ndef getGpuName():\n    gpu_name = torch.cuda.get_device_name(0).lower()\n    return gpu_name\n\nif __name__ == '__main__':\n    print(getGpuName())\n\n    # constants / parameters\n    MODEL_NAME = \"distilbert-base-uncased\"\n    DATASET_NAME = \"imdb\"\n    # dynamic batch size (kaggle vs my laptop)\n    BATCH_SIZE = dynamicBatchSize()  # kaggle supports batch size = 64 for T4 gpu\n\n    # enable some logs to debug properly, change wandb to offline mode for now\n    transformers.logging.set_verbosity_debug()  # Set to 'INFO' for fewer logs\n    wandb.init(mode=\"offline\")  # Logs only locally\n\n    # load imdb data\n    imdb_datasets_dict = datasets.load_dataset(DATASET_NAME)\n\n    # Drop unnecessary columns to speed up the process\n    isMyLaptop = \"nvidia geforce rtx 2060\" in getGpuName()\n\n    if isMyLaptop:\n        # my laptop is not meant to do actual bert training. just some quick runs to makesure my code is ok.\n        # else I need to debug the code in kaggle which will be a hassle\n        imdb_datasets_dict = DatasetDict({\n            \"train\": imdb_datasets_dict[\"train\"].select(range(25)),\n            # Select the first 25 entries from the train dataset\n            \"test\": imdb_datasets_dict[\"test\"].select(range(25))  # Select the first 25 entries from the test dataset\n        })\n    else:\n        imdb_datasets_dict = DatasetDict({\n            \"train\": imdb_datasets_dict[\"train\"],\n            \"test\": imdb_datasets_dict[\"test\"]\n        })\n\n    # check gpu availability\n    isGpuAvailable = torch.cuda.is_available()\n\n    # load tokenizer\n    distilBertTokenizer = DistilBertTokenizer.from_pretrained(pretrained_model_name_or_path=MODEL_NAME)\n\n\n    # preprocess / map data to tokenized_data\n    def tokenization_function(entry):\n        try:\n            value = entry[\"text\"]\n            tokenized_value = tokenized_value = distilBertTokenizer(text=value, padding=\"max_length\", truncation=True)\n            return tokenized_value\n        except Exception as x:\n            print(f\"Tokenization function error: {x = }\")\n            return None\n\n\n    tokenized_dataset_dict = imdb_datasets_dict.map(function=tokenization_function, batched=True)\n\n    # drop unnecessary table from tokenized dataset\n    print(\"creating tokenized_dataset\")\n    tokenized_dataset_dict = tokenized_dataset_dict.remove_columns([\"text\"])  # we don't need text column\n    tokenized_dataset_dict = tokenized_dataset_dict.rename_column(\"label\", \"labels\")  # cz huggingface wants y = labels\n    tokenized_dataset_dict.set_format(\"torch\")  # convert to pytorch objects\n\n    print(\"creating DataCollatorWithPadding\")\n    # Question: What is data collector? / what does it do?\n    # Data collator for padding batches dynamically\n    data_collator = DataCollatorWithPadding(tokenizer=distilBertTokenizer)\n\n    # load the bert model\n    bert_model = (DistilBertForSequenceClassification\n                  .from_pretrained(pretrained_model_name_or_path=MODEL_NAME, num_labels=2))\n    if isGpuAvailable:\n        bert_model = bert_model.to(\"cuda\")\n\n    # init the training_args\n    print(\"init training_args\")\n    training_args = TrainingArguments(\n        run_name=\"exp-bert-2\",\n        output_dir=\"./bert-imdb\",\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        logging_dir=\"./logs\"\n    )\n    # create trainer object\n\n    # Load desired metrics\n    # Load metrics\n    accuracy_metric = evaluate.load(\"accuracy\")\n    f1_metric = evaluate.load(\"f1\")\n    roc_auc_metric = evaluate.load(\"roc_auc\")\n\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=1)  # Get predicted class\n\n        positive_logits = logits[:,\n                          1]  # convert 2d array into 1d array like this: logits[0][1], logits[1][1], logits[2][1], ... ..., logits[n][1]\n        print(\"----- debug start ----\")\n        print(f\"{logits = }\")  # a 2d array.\n        print(f\"{labels = }\")  # 1d array\n        print(f\"{predictions = }\")  # 1d array\n        print(f\"{positive_logits = }\")\n        print(\"----- debug end ----\")\n\n        accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n        f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n        # roc_auc = roc_auc_metric.compute(prediction_scores=logits, references=labels)[\"roc_auc\"] # 2d array vs 1d array matrix dim mismatch\n        roc_auc = roc_auc_metric.compute(prediction_scores=positive_logits, references=labels)[\n            \"roc_auc\"]  # using positive_logits repairs the error\n\n        return {\n            \"accuracy\": accuracy,\n            \"f1\": f1,\n            \"roc_auc\": roc_auc\n        }\n\n\n    print(\"create trainer\")\n    trainer = Trainer(\n        model=bert_model,\n        args=training_args,\n        train_dataset=tokenized_dataset_dict[\"train\"],  # train\n        eval_dataset=tokenized_dataset_dict[\"test\"],  # validate\n        data_collator=data_collator,\n        compute_metrics=compute_metrics\n    )\n\n    # trainer.start, trainer.end\n    # the training!\n    print(\"trainer.train()!\")\n    trainer.train()  # this will fine tune the dataset for 3 epochs!\n    print(\"trainer.evaluate()!\")\n    trainer.evaluate()  # evaluate\n\n\n    # test_results = trainer.evaluate(test_dataset) # <-- this is the actual test\n\n    def predict_sentiment(text):\n        device = getCorrectDevice()\n        tokenized_text = distilBertTokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n        tokenized_text = {key: value.to(device) for key, value in tokenized_text.items()}\n\n        with torch.no_grad():\n            outputs = bert_model(**tokenized_text)\n\n        logits = outputs.logits\n        probabilities = F.softmax(logits, dim=1)  # Convert logits to probabilities\n        predicted_class = torch.argmax(probabilities, dim=1).item()  # Get class with max probability\n\n        return f\"Prediction: {'Positive' if predicted_class == 1 else 'Negative'}, Probabilities: {probabilities.tolist()}\"\n\n\n    print(\"predict_statement\")\n    print(predict_sentiment(\"I really loved this movie! It was fantastic.\"))\n    print(predict_sentiment(\"This was the worst movie I have ever seen.\"))\n    # make some predictions\n\n    # new topic: explain the bert model, ie why it works / does not work\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T18:42:17.295497Z","iopub.execute_input":"2025-03-22T18:42:17.295804Z"}},"outputs":[{"name":"stdout","text":"tesla t4\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2cac3e468cf4f149d8ae1a0a640eac0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d86f06602a0f4cfd9943be119864c132"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b824766c4eb405ab653b3a77e4ba934"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83d1e95f6b3f4eb6abd90b6ee765feb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44dea22142d64b5faae9e9db315e8bc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f28c3e1acde47c2aeed8974c83ff8f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af8aec4426e4d11a9d8e40c7b0f4bdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"222cb96821b74cb9b8d23c98d47ea64f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"833c95402b7948b3a5e1d946fd170dce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f733c92ab514921b59457f432b3250a"}},"metadata":{}},{"name":"stderr","text":"loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\nloading file chat_template.jinja from cache at None\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BertTokenizer'. \nThe class this function is called from is 'DistilBertTokenizer'.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efa448a6743d4058b35df5e6590f8e71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"139e2174762c4bb59f0f811ab9c7630b"}},"metadata":{}},{"name":"stdout","text":"creating tokenized_dataset\ncreating DataCollatorWithPadding\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2044e3d113c40249e40fb38087ba65b"}},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\nModel config DistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.47.0\",\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf88796f448d49b2acb08f2892017667"}},"metadata":{}},{"name":"stderr","text":"loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","output_type":"stream"},{"name":"stdout","text":"init training_args\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f1ad3ffab014849beae83d223f60903"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ef0173cd4b41dd86e7ef55b9b4dca7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/9.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62498954b6c84d5a97f86810cd5066b0"}},"metadata":{}},{"name":"stdout","text":"create trainer\ntrainer.train()!\n","output_type":"stream"},{"name":"stderr","text":"Currently training with a batch size of: 64\n***** Running training *****\n  Num examples = 25,000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Training with DataParallel so batch size has been adjusted to: 64\n  Total train batch size (w. parallel, distributed & accumulation) = 64\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1,173\n  Number of trainable parameters = 66,955,010\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1173' max='1173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1173/1173 45:45, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Roc Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.195391</td>\n      <td>0.924320</td>\n      <td>0.924291</td>\n      <td>0.977723</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.232200</td>\n      <td>0.191575</td>\n      <td>0.931440</td>\n      <td>0.931427</td>\n      <td>0.981022</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.096800</td>\n      <td>0.230199</td>\n      <td>0.934680</td>\n      <td>0.934680</td>\n      <td>0.981445</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 25000\n  Batch size = 64\n","output_type":"stream"},{"name":"stdout","text":"----- debug start ----\nlogits = array([[ 2.7276287 , -2.574289  ],\n       [ 1.5091671 , -1.5340017 ],\n       [ 1.7302245 , -1.5589788 ],\n       ...,\n       [-0.19799697,  0.08877969],\n       [-0.13899536, -0.06270194],\n       [-1.2416326 ,  1.104221  ]], dtype=float32)\nlabels = array([0, 0, 0, ..., 1, 1, 1])\npredictions = array([0, 0, 0, ..., 1, 1, 1])\npositive_logits = array([-2.574289  , -1.5340017 , -1.5589788 , ...,  0.08877969,\n       -0.06270194,  1.104221  ], dtype=float32)\n----- debug end ----\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./bert-imdb/checkpoint-391\nConfiguration saved in ./bert-imdb/checkpoint-391/config.json\nModel weights saved in ./bert-imdb/checkpoint-391/model.safetensors\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n\n***** Running Evaluation *****\n  Num examples = 25000\n  Batch size = 64\n","output_type":"stream"},{"name":"stdout","text":"----- debug start ----\nlogits = array([[ 3.2685633 , -3.0231888 ],\n       [ 2.1437113 , -2.0330033 ],\n       [ 2.6006508 , -2.2614734 ],\n       ...,\n       [-0.6448241 ,  0.5076494 ],\n       [-0.78823406,  0.5428132 ],\n       [-1.8470358 ,  1.789934  ]], dtype=float32)\nlabels = array([0, 0, 0, ..., 1, 1, 1])\npredictions = array([0, 0, 0, ..., 1, 1, 1])\npositive_logits = array([-3.0231888, -2.0330033, -2.2614734, ...,  0.5076494,  0.5428132,\n        1.789934 ], dtype=float32)\n----- debug end ----\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./bert-imdb/checkpoint-782\nConfiguration saved in ./bert-imdb/checkpoint-782/config.json\nModel weights saved in ./bert-imdb/checkpoint-782/model.safetensors\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bert-imdb/checkpoint-1173\nConfiguration saved in ./bert-imdb/checkpoint-1173/config.json\nModel weights saved in ./bert-imdb/checkpoint-1173/model.safetensors\n\n***** Running Evaluation *****\n  Num examples = 25000\n  Batch size = 64\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"----- debug start ----\nlogits = array([[ 3.535918  , -3.2899764 ],\n       [ 3.120323  , -2.93264   ],\n       [ 3.1935651 , -2.8434386 ],\n       ...,\n       [-0.19418688,  0.19878528],\n       [-1.7908183 ,  1.6664159 ],\n       [-2.6256824 ,  2.661802  ]], dtype=float32)\nlabels = array([0, 0, 0, ..., 1, 1, 1])\npredictions = array([0, 0, 0, ..., 1, 1, 1])\npositive_logits = array([-3.2899764 , -2.93264   , -2.8434386 , ...,  0.19878528,\n        1.6664159 ,  2.661802  ], dtype=float32)\n----- debug end ----\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./bert-imdb/checkpoint-1173\nConfiguration saved in ./bert-imdb/checkpoint-1173/config.json\nModel weights saved in ./bert-imdb/checkpoint-1173/model.safetensors\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n***** Running Evaluation *****\n  Num examples = 25000\n  Batch size = 64\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"trainer.evaluate()!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='327' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [327/391 03:15 < 00:38, 1.67 it/s]\n    </div>\n    "},"metadata":{}}],"execution_count":null}]}