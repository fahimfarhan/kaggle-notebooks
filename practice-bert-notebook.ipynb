{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n\"\"\"\n\nCoding Task:\n\n    Fine-tune BERT on a small dataset (e.g., IMDb movie reviews for sentiment classification).\n    Use transformers and Trainer from Hugging Face.\n\nHint: Use the datasets library to load IMDb:\n\"\"\"\nimport torch\nimport transformers\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import DistilBertTokenizer, DataCollatorWithPadding, DistilBertForSequenceClassification, TrainingArguments, \\\n    Trainer\nimport wandb\n\nMODEL_NAME = \"distilbert-base-uncased\" # \"bert-base-uncased\"\n\nif __name__ == '__main__':\n    isGpuAvailable = torch.cuda.is_available()\n    print(f\"{isGpuAvailable = }\")  # Should print: True\n    print(f\"{torch.cuda.device_count() = }\")  # Should print: 1 (if you have one GPU)\n    if isGpuAvailable:\n        print(f\"{torch.cuda.get_device_name(0) = }\")  # on my laptop, it should print: NVIDIA GeForce RTX 2060 Max-Q. on kaggle, it prints tesla t4\n    else:\n        print(\"No gpu found!\")\n\n    transformers.logging.set_verbosity_debug()  # Set to 'INFO' for fewer logs\n    wandb.init(mode=\"offline\")  # Logs only locally\n\n    imdb_dataset = load_dataset(\"imdb\")\n\n    imdb_dataset = DatasetDict({        # keep only what I need\n        \"train\": imdb_dataset[\"train\"],\n        \"test\": imdb_dataset[\"test\"]\n    })\n\n\n\n    print(f\"{imdb_dataset = }\")\n    # 2 columns, text, and label\n\n    # load tokenizer\n    tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n\n    # we need a tokenizer function. for now follow tutorial. java styled code organization later\n    def tokenize_function(example):\n        try:\n            return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n        except Exception as e:\n            print(f\"Error in tokenization: {e}\")\n            return None  # Or handle differently\n\n    tokenized_dataset = imdb_dataset.map(tokenize_function, batched=True)\n\n    print(f\"{tokenized_dataset = }\")\n    \"\"\"\n    tokenized_dataset = DatasetDict({\n        train: Dataset({\n            features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n            num_rows: 25000\n        })\n        test: Dataset({\n            features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n            num_rows: 25000\n        })\n        unsupervised: Dataset({\n            features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n            num_rows: 50000\n        })\n    })    \n    \"\"\"\n\n    # convert data to pytorch format\n    print(\"creating tokenized_dataset\")\n    tokenized_dataset = tokenized_dataset.remove_columns([\"text\"]) # we don't need text column\n    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\") # huggingface library wants y = labels\n    tokenized_dataset.set_format(\"torch\") # convert matrices into pytorch\n\n    print(\"creating DataCollatorWithPadding\")\n    # Data collator for padding batches dynamically\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  # Question: What is data collector? / what does it do?\n\n    # load the model\n    print(\"creating Model\")\n    model = (DistilBertForSequenceClassification\n             .from_pretrained(MODEL_NAME, num_labels = 2))\n\n    if isGpuAvailable:\n        print(\"before setting model to cuda\")\n        model = model.to(\"cuda\")  # <-- Ensure Trainer runs on GPU\n        print(\"after setting model to cuda\")\n\n    # define training arguments, and trainer\n    BATCH_SIZE = 32\n    print(\"creating train_args\")\n    train_args = TrainingArguments(\n        run_name=\"bert_imdb_experiment\",  # Set a custom run name to repair wandDb warning\n        output_dir=\"./bert-imdb\",\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        # device=\"cuda\" # looks like different version had this param\n    )\n\n    # question: what about optimizer, loss function?\n\n    # initialize the trainer\n    print(\"creating trainer\")\n    trainer = Trainer(\n        model=model,\n        args=train_args,\n        train_dataset=tokenized_dataset[\"train\"],\n        eval_dataset=tokenized_dataset[\"test\"],\n        # tokenizer=tokenizer, # tokenizer is deprecated. use data_collector, or process_class instead :/\n        data_collator=data_collator,\n        # device=\"cuda\" # looks like different version had this param\n    )\n\n    # the training!\n    print(\"trainer.train()!\")\n    trainer.train() # this will fine tune the dataset for 3 epochs!\n    print(\"trainer.evaluate()!\")\n    trainer.evaluate() # evaluate\n\n    def predict_sentiment(text):\n        tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n        with torch.no_grad():    # forgot the braces. \n            outputs = model(**tokenized_text)\n        logits = outputs.logits\n        prediction = torch.argmax(logits, dim=1).item()\n        return \"Positive\" if prediction == 1 else \"Negative\"\n\n    print(\"predict_statement\")\n    print(predict_sentiment(\"I really loved this movie! It was fantastic.\"))\n    print(predict_sentiment(\"This was the worst movie I have ever seen.\"))\n\n    pass\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-19T05:16:06.087733Z","iopub.execute_input":"2025-03-19T05:16:06.088033Z","iopub.status.idle":"2025-03-19T06:10:18.164892Z","shell.execute_reply.started":"2025-03-19T05:16:06.088008Z","shell.execute_reply":"2025-03-19T06:10:18.163634Z"}},"outputs":[{"name":"stdout","text":"isGpuAvailable = True\ntorch.cuda.device_count() = 2\ntorch.cuda.get_device_name(0) = 'Tesla T4'\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e08ab594134fc1b831c93e71d6a674"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bad8ab6a2467429a946a7cb557dcb5a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dce8af992c6b43868d7cc0168281d318"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbe54d3f7d76461b96a12312fee2b796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e246ba76716f4165856ccb192bff41dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"918dea98ce6f480aa1ce84e876e2fb3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06d27d6dfd0d498cb312281a837f33ea"}},"metadata":{}},{"name":"stdout","text":"imdb_dataset = DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2769e67bff6e4dab8e080b5079c840d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38bead5ef9ef46aebbe923c07d229f1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"443bcb517a574c72af0af424945182cc"}},"metadata":{}},{"name":"stderr","text":"loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\nloading file chat_template.jinja from cache at None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e51c7ee9cb7f4945a24bee517cf3c415"}},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.47.0\",\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ade59fe98b71444da72bbfa0d332466c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e1b27027ce647bbb4a412c23166e919"}},"metadata":{}},{"name":"stdout","text":"tokenized_dataset = DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 25000\n    })\n})\ncreating tokenized_dataset\ncreating DataCollatorWithPadding\ncreating Model\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\nModel config DistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.47.0\",\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"259020d8ae654a8aab6cb1f8f7d97c61"}},"metadata":{}},{"name":"stderr","text":"loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"before setting model to cuda\n","output_type":"stream"},{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","output_type":"stream"},{"name":"stdout","text":"after setting model to cuda\ncreating train_args\ncreating trainer\ntrainer.train()!\n","output_type":"stream"},{"name":"stderr","text":"Currently training with a batch size of: 64\n***** Running training *****\n  Num examples = 25,000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Training with DataParallel so batch size has been adjusted to: 64\n  Total train batch size (w. parallel, distributed & accumulation) = 64\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1,173\n  Number of trainable parameters = 66,955,010\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1173' max='1173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1173/1173 44:35, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.203102</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.232600</td>\n      <td>0.184285</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.098400</td>\n      <td>0.235509</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 25000\n  Batch size = 64\nSaving model checkpoint to ./bert-imdb/checkpoint-391\nConfiguration saved in ./bert-imdb/checkpoint-391/config.json\nModel weights saved in ./bert-imdb/checkpoint-391/model.safetensors\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n\n***** Running Evaluation *****\n  Num examples = 25000\n  Batch size = 64\nSaving model checkpoint to ./bert-imdb/checkpoint-782\nConfiguration saved in ./bert-imdb/checkpoint-782/config.json\nModel weights saved in ./bert-imdb/checkpoint-782/model.safetensors\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bert-imdb/checkpoint-1173\nConfiguration saved in ./bert-imdb/checkpoint-1173/config.json\nModel weights saved in ./bert-imdb/checkpoint-1173/model.safetensors\n\n***** Running Evaluation *****\n  Num examples = 25000\n  Batch size = 64\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bert-imdb/checkpoint-1173\nConfiguration saved in ./bert-imdb/checkpoint-1173/config.json\nModel weights saved in ./bert-imdb/checkpoint-1173/model.safetensors\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n***** Running Evaluation *****\n  Num examples = 25000\n  Batch size = 64\n","output_type":"stream"},{"name":"stdout","text":"trainer.evaluate()!\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [391/391 03:45]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"predict_statement\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-9e0b41337d0a>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict_statement\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I really loved this movie! It was fantastic.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This was the worst movie I have ever seen.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-9e0b41337d0a>\u001b[0m in \u001b[0;36mpredict_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: __enter__"],"ename":"AttributeError","evalue":"__enter__","output_type":"error"}],"execution_count":1}]}
