{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport csv\nimport os\n\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nfrom datasets import load_dataset\nfrom datasets import load_dataset\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import Callback\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import IterableDataset\nfrom torchmetrics.classification import (\n    Accuracy,\n    Precision,\n    Recall,\n    F1Score,\n    MulticlassAUROC,\n)\nfrom transformers import AutoTokenizer\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"Too many dataloader workers:.*\")\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert_small\")\n\ndef tokenize_bangla_text(\n        text,\n        tokenizer,\n        max_len=tokenizer.vocab_size\n):\n    tokens = tokenizer(\n        text,\n        padding='max_length',\n        truncation=False,\n        max_length=max_len,\n        return_tensors=\"pt\"\n    )\n    return tokens[\"input_ids\"]\n\nclass CudaMemoryCleanupCallback(Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()  # optional but sometimes helps in multi-process scenarios\n        # print(\"[INFO] Emptied CUDA cache at epoch end.\")\n\n\nclass BanglaTextDataset(IterableDataset):\n    def __init__(self, hf_dataset, tokenizer, max_len=100):  # todo: check Max length\n        self.dataset = hf_dataset\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __iter__(self):\n        for item in self.dataset:\n            sentence = item[\"X\"]\n            tokenized_sentence = tokenize_bangla_text(sentence, tokenizer).squeeze(0)\n            # tokenized_sentence = tokenize_bangla_text(sentence, tokenizer)\n\n            label = item[\"y\"]\n            # tokenized_label = tokenize_bangla_text(label, tokenizer).squeeze(0)\n\n            # tmp_label = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"গাই\"))[-1]  # Get the last/only token's id\n            # tmp_label = torch.tensor([tmp_label], dtype=torch.long)  # Convert to tensor\n\n            tokenized_label = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(label))[\n                -1]  # Get the last/only token's id\n            tokenized_label = torch.tensor([tokenized_label], dtype=torch.long)  # Convert to tensor\n\n            yield tokenized_sentence, tokenized_label\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass NextWordPredictMetrics:\n    def __init__(self, vocab_size):\n        self.vocab_size = vocab_size\n        self.accuracy = Accuracy(task=\"multiclass\", num_classes=vocab_size, average=\"macro\").to(DEVICE)\n        self.precision = Precision(task=\"multiclass\", num_classes=vocab_size, average=\"macro\").to(DEVICE)\n        self.recall = Recall(task=\"multiclass\", num_classes=vocab_size, average=\"macro\").to(DEVICE)\n        self.f1 = F1Score(task=\"multiclass\", num_classes=vocab_size, average=\"macro\").to(DEVICE)\n        # self.auc = MulticlassAUROC(num_classes=vocab_size).to(DEVICE)\n\n    def update(self, preds, labels):\n        self.accuracy.update(preds, labels)\n        self.precision.update(preds, labels)\n        self.recall.update(preds, labels)\n        self.f1.update(preds, labels)\n        # try:\n        #     # AUC needs probs, not hard preds\n        #     self.auc.update(preds, labels)\n        # except Exception:\n        #     pass  # skip AUC for unstable batches\n\n    def compute(self):\n        return {\n            \"accuracy\": self.accuracy.compute(),\n            \"precision\": self.precision.compute(),\n            \"recall\": self.recall.compute(),\n            \"f1\": self.f1.compute(),\n            # \"auc\": self.auc.compute(),\n        }\n\n    def reset(self):\n        self.accuracy.reset()\n        self.precision.reset()\n        self.recall.reset()\n        self.f1.reset()\n        # self.auc.reset()\n\nclass NextWordPredictor(nn.Module):\n    def __init__(self, *,\n                 vocab_size: int,\n                 embedding_dim: int = 128,\n                 hidden_dim: int = 128,\n                 dropout_rate: float = 0.2):\n        \"\"\"\n        Args:\n            vocab_size (int): Size of the vocabulary\n            embedding_dim (int): Dimension of word embeddings\n            hidden_dim (int): Dimension of hidden states in GRU\n            dropout_rate (float): Dropout probability\n        \"\"\"\n        super(NextWordPredictor, self).__init__()\n\n        self.embedding = nn.Embedding(\n            num_embeddings=vocab_size,\n            embedding_dim=embedding_dim\n        )\n\n        self.gru1 = nn.GRU(\n            input_size=embedding_dim,\n            hidden_size=hidden_dim,\n            batch_first=True\n        )\n\n        self.gru2 = nn.GRU(\n            input_size=hidden_dim,\n            hidden_size=hidden_dim,\n            batch_first=True\n        )\n\n        self.linear1 = nn.Linear(\n            in_features=hidden_dim,\n            out_features=hidden_dim\n        )\n\n        self.linear2 = nn.Linear(\n            in_features=hidden_dim,\n            out_features=vocab_size\n        )\n\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length)\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, vocab_size)\n        \"\"\"\n        embedded = self.embedding(x)\n\n        out, _ = self.gru1(embedded)\n        out, _ = self.gru2(out)\n\n        out = out[:, -1, :]  # Get the last output\n\n        out = self.dropout(self.relu(self.linear1(out)))\n        out = self.linear2(out)\n\n        return out\n\n\n\ndef pretty_print_metrics(metrics: dict, stage: str = \"\"):\n    metrics_str = f\"\\n📊 {stage} Metrics:\\n\" + \"\\n\".join(\n        f\"  {k:>15}: {v:.4f}\" if v is not None else f\"  {k:>15}: N/A\"\n        for k, v in metrics.items()\n    )\n    print(metrics_str)\n\n\nclass NextWordPredictorModule(pl.LightningModule):\n    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, lr=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.model = NextWordPredictor(\n            vocab_size=self.hparams.vocab_size,\n            embedding_dim=self.hparams.embed_dim,\n            hidden_dim=self.hparams.hidden_dim\n        )\n        self.criterion = nn.CrossEntropyLoss()\n        # Use your metrics class\n        self.train_metrics = NextWordPredictMetrics(vocab_size)\n        self.val_metrics = NextWordPredictMetrics(vocab_size)\n        self.test_metrics = NextWordPredictMetrics(vocab_size)\n        pass\n\n    def forward(self, x):\n        return self.model(x)\n\n    def compute_loss_and_metrics(self, batch, metric_obj, stage_name):\n        input_ids, labels = batch\n\n        labels = labels.squeeze(1)  # from shape (8, 1) → (8,)\n        # Then in your training loop:\n        logits = self(input_ids)  # Should now be [batch_size, sequence_length, vocab_size]\n\n        # print(f\"{labels.shape = }\")\n        # print(f\"{logits.shape = }\")\n\n        loss = self.criterion(logits, labels)\n        preds = torch.argmax(logits, dim=-1)\n        metric_obj.update(preds, labels)\n\n        self.log(f\"{stage_name}_loss\", loss, prog_bar=True)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        return self.compute_loss_and_metrics(batch, self.train_metrics, \"train\")\n\n    def validation_step(self, batch, batch_idx):\n        return self.compute_loss_and_metrics(batch, self.val_metrics, \"val\")\n\n    def test_step(self, batch, batch_idx):\n        return self.compute_loss_and_metrics(batch, self.test_metrics, \"test\")\n\n    def on_validation_epoch_end(self):\n        metrics = self.val_metrics.compute()\n        pretty_print_metrics(metrics, \"eval\")\n        self.val_metrics.reset()\n\n    def on_test_epoch_end(self):\n        metrics = self.test_metrics.compute()\n        pretty_print_metrics(metrics, \"test\")\n        self.test_metrics.reset()\n\n    def on_train_epoch_end(self):\n        metrics = self.train_metrics.compute()\n        pretty_print_metrics(metrics, \"train\")\n        self.train_metrics.reset()\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\ndef start():\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n    # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n    # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n    # Paths\n    INPUT_CSV = \"/kaggle/input/over-11500-bangla-news-for-nlp/Bangla_news.csv\"\n    PREPROCESSED_CSV = \"/kaggle/working/preprocessed.csv\"\n\n    TRAIN_CSV = \"/kaggle/working/train.csv\"\n    VAL_CSV = \"/kaggle/working/val.csv\"\n    TEST_CSV = \"/kaggle/working/test.csv\"\n\n    # Load the dataset in streaming mode\n    dataset = load_dataset('csv', data_files=INPUT_CSV, streaming=True)\n\n    # Open output file\n    with open(PREPROCESSED_CSV, 'w') as outfile:\n        # Write header\n        outfile.write('X,y\\n')\n\n        # Process each row\n        for row in dataset['train']:\n            # Get only the content column\n            sentence = row['content'].strip()\n            words = sentence.split()\n\n            # Generate input-output pairs\n            for i in range(len(words) - 1):\n                X = ' '.join(words[:i + 1])\n                y = words[i + 1]\n                outfile.write(f'\"{X}\",\"{y}\"\\n')\n\n    print(\"Preprocessing complete. Output saved to 'output.csv'.\")\n\n    # Open files for writing\n    train_file = open(TRAIN_CSV, 'w', newline='', encoding='utf-8')\n    val_file = open(VAL_CSV, 'w', newline='', encoding='utf-8')\n    test_file = open(TEST_CSV, 'w', newline='', encoding='utf-8')\n\n    train_writer = csv.writer(train_file)\n    val_writer = csv.writer(val_file)\n    test_writer = csv.writer(test_file)\n\n    # Load streaming dataset\n    dataset = load_dataset(\"csv\", data_files=PREPROCESSED_CSV, streaming=True)[\"train\"]\n\n    # Read header from first row\n    first_row = next(iter(dataset))\n    header = list(first_row.keys())\n    print(f\"{header = }\")\n    train_writer.writerow(header)\n    val_writer.writerow(header)\n    test_writer.writerow(header)\n\n    # Write first row\n    row_values = [first_row[col] for col in header]\n    train_writer.writerow(row_values)\n\n    # Counters\n    import random\n\n    train_count, val_count, test_count = 0, 0, 0\n\n    MAX_DATASET_SIZE = 1_000 # 50_000\n    for i, row in enumerate(dataset):\n        if i == MAX_DATASET_SIZE:\n            # to comply with kaggle's 12 hour run limit\n            break\n        row_values = [row[col] for col in header]\n        rand_val = random.random()\n        if rand_val < 0.8:\n            train_writer.writerow(row_values)\n            train_count += 1\n        elif rand_val < 0.9:\n            val_writer.writerow(row_values)\n            val_count += 1\n        else:\n            test_writer.writerow(row_values)\n            test_count += 1\n\n    train_file.close()\n    val_file.close()\n    test_file.close()\n\n    print(f\"Train: {train_count}, Val: {val_count}, Test: {test_count}\")\n\n    train_ds = load_dataset(\"csv\", data_files=TRAIN_CSV, streaming=True)[\"train\"]\n    eval_ds = load_dataset(\"csv\", data_files=VAL_CSV, streaming=True)[\"train\"]\n    test_ds = load_dataset(\"csv\", data_files=TEST_CSV, streaming=True)[\"train\"]\n\n\n\n\n    tmp_text = \"আমি বাংলায় গান গাই\"\n    tmp_tokenized = tokenize_bangla_text(tmp_text, tokenizer)\n    # print(tmp_tokenized)\n    print(len(tmp_tokenized))\n    # Output: [101, 2485, 2262, 3764, 1234, 102, 0, 0, 0, 0, ...]\n\n    # run a single string into the model:\n    tmp_text = \"আমি বাংলায় গান গাই\"\n    tmp_tokenized_input_ids = tokenize_bangla_text(tmp_text, tokenizer)\n    tmp_label1 = tokenize_bangla_text(\"গাই\", tokenizer)\n    print(f\"{tmp_label1 = }\")\n\n    tmp_label = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"গাই\"))[-1]  # Get the last/only token's id\n    tmp_label = torch.tensor([tmp_label], dtype=torch.long)  # Convert to tensor\n\n    # tmp_input_tensor = torch.tensor(tmp_tokenized_input_ids).unsqueeze(0)  # shape: (1, seq_len)\n    tmp_input_tensor = tmp_tokenized_input_ids\n    total_words = tokenizer.vocab_size  # or len(tokenizer.get_vocab())\n\n    tmp_model = NextWordPredictor(vocab_size=total_words)\n\n    tmp_output = tmp_model.forward(tmp_input_tensor)  # shape: (1, total_words)\n    print(f\"{tmp_input_tensor.shape = }\")\n    # print(f\"{tmp_output = }\")\n    print(f\"{tmp_output.shape = }\")  # Should be: (1, total_words)\n    print(f\"{tmp_output.size = }\")\n\n    print(f\"{tmp_output = }\")\n    print(f\"{tmp_label = }\")\n    print(f\"{tmp_label.shape = }\")\n\n    tmp_criterion = nn.CrossEntropyLoss()\n    tmp_loss = tmp_criterion(tmp_output, tmp_label)\n    print(f\"{tmp_loss = }\")\n\n    bangla_train_ds = BanglaTextDataset(hf_dataset=train_ds, tokenizer=tokenizer)\n    bangla_eval_ds = BanglaTextDataset(hf_dataset=eval_ds, tokenizer=tokenizer)\n    bangla_test_ds = BanglaTextDataset(hf_dataset=test_ds, tokenizer=tokenizer)\n\n    MY_BATCH_SIZE = 16 # 16 ok\n\n    train_dataloader = DataLoader(bangla_train_ds, batch_size=MY_BATCH_SIZE, num_workers=3)\n    eval_dataloader = DataLoader(bangla_eval_ds, batch_size=MY_BATCH_SIZE, num_workers=3)\n    test_dataloader = DataLoader(bangla_test_ds, batch_size=MY_BATCH_SIZE, num_workers=3)\n\n    vocab_size = tokenizer.vocab_size\n\n    ckpt_path = \"nextWordpPedictionModule.ckpt\"\n    pth_path = \"nextWordpPedictionModel.pth\"\n\n    module = NextWordPredictorModule(\n            vocab_size=vocab_size,\n            embed_dim=64,\n            hidden_dim=64, \n            lr=1e-3\n        )\n\n    # if os.path.exists(ckpt_path):\n    #     print(\"🔄 Loading full LightningModule from .ckpt\")\n    #     module = NextWordPredictorModule.load_from_checkpoint(ckpt_path, vocab_size=vocab_size)\n    # else:\n    #     module = NextWordPredictorModule(\n    #         vocab_size=vocab_size,\n    #         embed_dim=64, \n    #         hidden_dim=64, \n    #         lr=1e-3\n    #     )\n\n    trainer = Trainer(\n        max_epochs=3,\n        accelerator=\"auto\",\n        callbacks=[CudaMemoryCleanupCallback()],\n    )\n    try:\n        trainer.fit(module, train_dataloaders=train_dataloader, val_dataloaders=eval_dataloader)\n    except Exception as x:\n        print(x)\n    finally:\n        torch.save(module.model.state_dict(), pth_path)\n        trainer.save_checkpoint(ckpt_path)\n\n    trainer.test(module, dataloaders=test_dataloader)\n    pass\n\n\nif __name__ == '__main__':\n    start()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-11T13:48:57.647675Z","iopub.execute_input":"2025-06-11T13:48:57.647984Z","iopub.status.idle":"2025-06-11T13:49:18.104225Z","shell.execute_reply.started":"2025-06-11T13:48:57.647957Z","shell.execute_reply":"2025-06-11T13:49:18.103566Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/over-11500-bangla-news-for-nlp/Bangla_news.csv\n/kaggle/input/bangla-newspaper-dataset/data_v2/data_v2.json\n/kaggle/input/bangla-newspaper-dataset/data/data.json\nPreprocessing complete. Output saved to 'output.csv'.\nheader = ['X', 'y']\nTrain: 795, Val: 98, Test: 107\n1\ntmp_label1 = tensor([[   2, 3893,    3,  ...,    0,    0,    0]])\ntmp_input_tensor.shape = torch.Size([1, 32000])\ntmp_output.shape = torch.Size([1, 32000])\ntmp_output.size = <built-in method size of Tensor object at 0x7bd11dca9a30>\ntmp_output = tensor([[-0.0042,  0.0964, -0.1573,  ..., -0.0762, -0.0253, -0.0860]],\n       grad_fn=<AddmmBackward0>)\ntmp_label = tensor([3893])\ntmp_label.shape = torch.Size([1])\ntmp_loss = tensor(10.2884, grad_fn=<NllLossBackward0>)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"Too many dataloader workers: 3 (max is dataset.num_shards=1). Stopping 2 dataloader workers.\n","output_type":"stream"},{"name":"stdout","text":"\n📊 eval Metrics:\n         accuracy: 0.0000\n        precision: 0.0000\n           recall: 0.0000\n               f1: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Too many dataloader workers: 3 (max is dataset.num_shards=1). Stopping 2 dataloader workers.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0505f9a0f3544ffba2137894f228b75"}},"metadata":{}},{"name":"stdout","text":"CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.83 GiB is free. Process 2576 has 14.06 GiB memory in use. Of the allocated memory 7.70 GiB is allocated by PyTorch, and 6.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"name":"stderr","text":"Too many dataloader workers: 3 (max is dataset.num_shards=1). Stopping 2 dataloader workers.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaad426fb6f24056a4dfaf7c7da71856"}},"metadata":{}},{"name":"stdout","text":"\n📊 test Metrics:\n         accuracy: 0.0000\n        precision: 0.0000\n           recall: 0.0000\n               f1: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   10.370012283325195    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    10.370012283325195     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}}],"execution_count":14}]}